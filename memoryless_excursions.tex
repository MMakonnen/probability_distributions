%\documentclass[a4paper,14pt]{extarticle}
\documentclass[a4paper,12pt]{article}

%\usepackage[all-solutions-at-end]{optional}
\usepackage[check]{optional}

\usepackage{preamble}

\opt{check}{
%\AtBeginEnvironment{exercise}{\clearpage}
\AtEndEnvironment{exercise}{\clearpage}
}

\opt{all-solutions-at-end}{
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}

\title{Memoryless excursions\\
Notes on joint distributions }
\author{Nicky van Foreest}


\begin{document}
\maketitle
\tableofcontents



\section{A confusing problem}
\label{sec:confusing-problem}

Write $M=\max{X,Y}$ and $L=\min{X,Y}$ for given r.v.s $X$ and $Y$.
It is simple to see that when $X$ and $Y$  have the same distribution 
\begin{equation}
\E L + \E M = \E{L+M}=\E{X+Y} = 2\E X.
\end{equation}
From this, we have that 
\begin{equation}
  \label{eq:6}
\E M = 2\E X - \E L.
\end{equation}
Next, when $X$ and $Y$ are memoryless and iid, it is  tempting to think that
\begin{equation}
  \label{eq:7}
\E M = \E L  + \E X,
\end{equation}
because of  the following interpretation.
There are two machines each working on a job in parallel.
Let $X$ and $Y$ be the production times at either machine.
Then, we first wait until the first job finishes; this time is evidently $L=\min{X, Y}$.
Then, due to memoryless, the service time of the remaining job `restarts'; this takes an expected time $\E X$ to complete.

Using~\cref{eq:6} and \cref{eq:7}  we can solve for $\E M$ and $\E L$.
Adding the two equations and canceling $\E L$ at both sides gives $2\E M = 3 \E X $, hence:
\begin{align}
\E M &= \frac 3 2 \E X,\label{eq:35}\\
\E L &= \E M - \E X = \frac 1 2 \E X.\label{eq:36}
\end{align}

However, as we will see in the first set of exercises below, this is \emph{ not true for discrete} memoryless r.v.s, i.e., when $X, Y \sim \Geo{p}$, while it is true when $X, Y \sim \Exp{\lambda}$, cf., BW Ex.5.6.5.\footnote{BH: Blitzstein and Hwang}
In the exercises below I try to help you find out why this is so, and how to adapt~\cref{eq:6} and/or \cref{eq:7} so that they also hold when $X,Y\sim\Geo{p}$.

Actually, these exercises serve a few much more general goals. They are meant to
\begin{enumerate}
\item become familiar with all concepts of Section 7.1 of BH.
\item  provide a recap of concepts and results of  earlier chapters in the book,
\item  motivate to read (and memorize) the material of BH A1, A.2.5, A.6, A.7, A.8, A.9 and A.10.
\end{enumerate}
Once you solved all these exercises below, the exercises of BH will be considerably much simpler.
All exercises have solutions, and focus on computational steps. I will provide motivation and  explanations in the online lectures (on youtube).


I tend to use the following step-wise approach:
\begin{enumerate}
\item To compute probabilities of functions of r.v.s: use the fundamental bridge and the formula
  \begin{equation}
    \label{eq:55}
    \begin{split}
\P{g(X, Y) \in A} 
&= \E{\1{g(X,Y)\in A}} \\
&= \iint \1{g(x,y) \in A} f_{X,Y}(x,y) \d x \d y \\
&= \iint_{g^{-1}(A)} f_{X,Y}(x, y) \d x \d y,\\
&= \iint_{\{(x,y): g(x,y) \in A\}} f_{X,Y}(x,y) \d x \d y,
    \end{split}
  \end{equation}
or the discrete variant of this, and then do the integrals.
Conceptually this approach is easy, but carrying out the integration is not always simple.
However, the advantage is that I don't need special tricks to solve the problem; it is just (quite a bit of) straightforward work.
With practice, this approach always works.
\item The joint CDF $F_{X,Y}$ follows from step 1, because $F_{X,Y}(x,y) = \P{X\leq x, Y\leq y}$.
\item To get the joint density $f_{X,Y}$ say, I often first compute the joint distribution $F_{X,Y}$ and then take partial derivatives.
  This approach is `trick-free', but requires at times the hard work of solving the integrals to find $F_{X,Y}$.
\end{enumerate}

In the exercises below we first try to see which of~\cref{eq:6} and \cref{eq:7} is (and is not) true for memoryless discrete r.v.s, then we consider memoryless continuous r.v.s. Once we have dealt with these two cases, we can  resolve the confusing problem.


\subsection{Discrete memoryless r.v.s}
\label{sec:set-1}


Let $X,Y$ be $\iid \sim \Geo{p}$. Write $q = 1-p$.

\begin{exercise}
As a recap of earlier chapters of BH, show that
\begin{align}
  \label{eq:9}
\P{X>j} &= q^{j+1}, &\P{X\geq j} &=q^j,& \E{X}&=\frac q{1-q}.
 \end{align}
What is the domain of $X$?
\begin{solution}
Of course $X \in \{0, 1, 2, \ldots\}$.

Here are a couple of tricks with recursions; this is how I rederive the properties of a geometric r.v.
\begin{align}
  \label{eq:8}
\P{X>0} &= \P{\textrm{failure}} = q\\
\P{X>j} &= q \P{X>j-1} \implies \P{X>j}=q^{j}\P{X>0} = q^{j+1}.\\
\P{X=j} &= \P{X>j-1} - \P{X>j} = q^{j}-q^{j+1} = (1-q)q^{j} = p q^{j}.\\
\E X &= p\cdot 0 + q(1+\E X) \implies \E X = q/(1-q) = q/p.
\end{align}

Now the same answers, but with the regular method. 
  \begin{align}
    \label{eq:5}
\P{X>j}    
&= \sum_{i=j+1}^{\infty} \P{X=i} \\
&= p\sum_{i=j+1}^{\infty} q^{i} \\
&= p\sum_{i=0}^{\infty} q^{j+1+i} \\
&= pq^{j+1}\sum_{i=0}^{\infty} q^{i} \\
&= pq^{j+1}\frac 1 {1-q} = p q^{j+1} \frac 1 p = q^{j+1}.
  \end{align}
From this,
  \begin{align}
    \label{eq:56}
  \P{X\geq j}  &= \P{X>j-1} = q^{j}, \\
  \end{align}

The use of indicator variables is particularly important:
  \begin{align}
    \label{eq:58}
\E X 
&= \sum_{i=0}^{\infty} i\P{X=i} \\
&=  \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \1{j<i} \P{X=i} \\
&=  \sum_{j=0}^{\infty} \sum_{i=0}^{\infty} \1{j<i} \P{X=i} \\
&=  \sum_{j=0}^{\infty} \sum_{i=j+1}^{\infty}  \P{X=i} \\
&=  \sum_{j=0}^{\infty}  \P{X>j} \\
&=  \sum_{j=0}^{\infty}  q^{j+1}\\
&=  q \sum_{j=0}^{\infty}  q^{j}\\
&=  q/(1-q) = q/p.
  \end{align}

\end{solution}
\end{exercise}

\begin{exercise}
Check that $X$, and hence $Y$, is memoryless.
\begin{hint}
What is the definition of memorylessness for discrete r.v.s?  
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:13}
\P{X\geq n+m\given X\geq m} 
&= \frac{ \P{X\geq n+m,  X\geq m}}{\P{X\geq m}} \\
&= \frac{ \P{X\geq n+m}}{\P{X\geq m}} \\
&= \frac{ q^{n+m}}{q^m} \\
&= q^n= \P{X\geq n}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:12}
Show that $\P{L\geq i}=q^{2i}$ and conclude that $L\sim\Geo{1-q^{2}}$. Just to keep you sharp: what is the domain of $L$?
\begin{solution}
  \begin{align}
    \label{eq:12}
\P{L\geq k} 
&= \sum_{ij} \1{\min{i,j}\geq k} \P{X=i, Y=j}\\
&= \sum_{i\geq k} \sum_{j\geq k} \P{X=i}\P{Y=j}\\
&=  \P{X\geq k}\P{Y\geq k} = q^{k} q^{k} = q^{2k}.
  \end{align}
$\P{L>i}$ has the same form as $\P{X>i}$, but now with $q^{2i}$ rather than $q^{i}$.
\end{solution}
\end{exercise}

As henceforth all r.v.s we consider are non-negative, I'll drop the question to specify the domain of the r.v.s. However, in general you should not forget to specify this. 



\begin{exercise}\label{ex:3}
Show that  $\E L=q^{2}/(1-q^{2})$.
\begin{hint}
$X\sim \Geo{1-q} \implies \E X = q/(1-q)$. Now use that $L\sim\Geo{1-q^{2}}$. 
\end{hint}
\begin{solution}
  Immediate from the hint and~\cref{ex:12}. 
\end{solution}
\end{exercise}

\begin{exercise}
Show that 
\begin{equation}
  \label{eq:39}
\E L + \E X = \frac q{1-q}\frac{1+2q}{1+q}.
\end{equation}
\begin{hint}
Use that $1-q^2=(1-q)(1+q)$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:2}
\E L     + \E X 
&= \frac{q^{2}}{1-q^2} + \frac{q}{1-q} \\
&= \frac q{1-q}\left(\frac{q}{1+q} + 1\right)\\
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Supposing that~\cref{eq:6} is true, show that
\begin{equation}
  \label{eq:10}
\E M = 2\E X  -\E L = \frac q{1-q} \frac{2+q}{1+q}.
\end{equation}
Show also that this is not the same as~\cref{eq:39} (unless $q=0$).
Conclude that this violates the simultaneous truth of~\cref{eq:6} and \cref{eq:7}.
\begin{solution}
  \begin{align}
    \label{eq:11}
\E M 
&= 2\E X - \E L \\
&= 2\frac q{1-q}  - \frac{q^2}{1-q^2}\\
&= \frac q{1-q}\left(2  - \frac{q}{1+q}\right)\\
&= \frac q{1-q}\left(\frac{2+2q}{1+q}  - \frac{q}{1+q}\right)\\
&= \frac q{1-q}\frac{2+q}{1+q}.
  \end{align}
\end{solution}
\end{exercise}


So, something is wrong.
I believe that~\cref{eq:6} is correct, but then~\cref{eq:7} must be wrong.
But why?
And, perhaps I am mistaken, and \cref{eq:7} is correct, and the other is not.

In fact, to convince myself that~\cref{eq:7} is indeed wrong, I pursued three ideas to check~\cref{eq:10}, hence~\cref{eq:6}

\begin{exercise}\label{ex:1}
Idea 1: Show that for the PMF of $M$: 
\begin{equation}
  \label{eq:34}
p_M(k) = \P{M=k} = 2 pq^{k}(1-q^{k}) + p^2q^{2k}.
\end{equation}
Then use this to show~\cref{eq:10}.
\begin{solution}
  \begin{align}
    \label{eq:14}
\P{M=k}     
&= \P{\max{X, Y} = k} \\
&=p^{2}\sum_{ij}\1{\max{i,j} =k} q^i q^j\\
&=2 p^{2}\sum_{ij}\1{i=k}\1{j < k} q^i q^j + p^{2}\sum_{ij}\1{i=j=k} q^{i} q^j \\
&=2 p^{2}q^{k}\sum_{j < k} q^j + p^{2}q^{2k}\\
&=2 p^{2}q^{k}\frac{1-q^{k}}{1-q} +  p^{2}q^{2k}\\
&=2 pq^{k}(1-q^{k}) + p^{2}q^{2k}\\
&=2 pq^{k} +(p^{2}-2p) q^{2k} \label{eq:18}\\
&=2 \P{X=k} - \P{L=k},
  \end{align}
where I use that $p^{2}-2p = p(p-2) = (1-q)(1-q-2)=-(1-q)(1+q)=-(1-q^{2})$.
  \begin{align}
    \label{eq:15}
\E M 
&= \sum_k k \P{M=k}     \\
&=  \sum_{k}k (2\P{X=k} - \P{L=k}) \\
&= 2 \E X  - \E L.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Idea 2: Use  $\P{M\leq k} = (\P{X\leq k})^{2}$  to check~\cref{eq:34}.
\begin{hint}
$\P{M=k} = \P{M\leq k} - \P{M\leq k-1}$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:16}
\P{M\leq k}   
&= \P{X\leq k, Y\leq k} \\
&= \P{X\leq k}\P{Y\leq k} \\
&= (1-\P{X> k})(1-\P{Y> k}) \\
&= (1-q^{k+1})^{2}.\\
\P{M=k} &= \P{M\leq k} - \P{M\leq k-1}\\
&= 1-2 q^{k+1} + q^{2k+2} - (1 - 2q^{k} + q^{2k})\\
&= 2 q^{k}(1-q) + q^{2k}(q^{2}-1) \\
&= 2 \P{X=k}  - q^{2k}(1-q^{2}) \\
&= 2 \P{X=k}  - \P{L=k}.
  \end{align}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:2}
Idea 3a: show that  the  joint PMF
\begin{equation}
p_{L,M}(i,j) = \P{L=i, M=j} = 2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}.
\end{equation}
\begin{solution}
  \begin{align}
    \label{eq:17}
\P{L=i, M=j}    
&= 2\P{X=i, Y=j}\1{j>i} + \P{X=Y=i}\1{i=j} \\
&= 2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:8}
Idea 3b: Use~\cref{ex:2} to compute the marginal PMFs of $M$ and of $L$. 
\begin{hint}
For the first, marginalize out $L$, for the second, marginalize out $M$.
\end{hint}
\begin{solution}
  \begin{align}
\P{M=j}    
&= \sum_{i} \P{L=i, M=j} \\
 &= \sum_{i} (2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}) \\
 &= 2p^2q^j\sum_{i=0}^{j-1} q^{i}+ p^{2}q^{2j} \label{eq:38}\\
 &= 2pq^j (1-q^{j})+ p^{2}q^{2j} \\
 &= 2pq^j + (p^{2}-2p) q^{2j},
  \end{align}
which is equal to \cref{eq:18}.
  \begin{align}
\P{L=i}    
&= \sum_{j} \P{L=i, M=j} \\
 &= \sum_{j} (2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}) \\
 &= 2p^2q^i\sum_{j=i+1}^{\infty} q^{j}+ p^{2}q^{2i} \\
 &= 2p^{2}q^{2i+1} \sum_{j=0}^{\infty}q^{j}+ p^{2}q^{2i} \\
 &= 2pq^{2i+1} + p^{2} q^{2i}\\
 &= pq^{2i}(2q+p)\\
 &= (1-q)q^{2i}(q + 1), \quad p+q=1,\\
 &= (1-q^{2})q^{2i},
  \end{align}
and this we found earlier.
\end{solution}
\end{exercise}


Finally, BH Ex.5.6.5 use that the r.v.
$M-L$ is independent of $L$ when $X, Y\sim\Exp{\lambda}$ and independent.
But this leads to our confusing problem, so perhaps my intuition about memorylessness is not ok.
To check this, I solved  the following exercise.  

\begin{exercise}\label{ex:10}
Show that the events $\{L=i\}$ and $\{M>L\}$ are independent when $X,Y \sim \Geo{p}$.
\begin{hint}
Show that $\P{L=i \given M>L} = \P{L=i}$; recall, to show independence we need the probability (measure) $\P{\cdot}$.

Use the expression $\P{g(X,Y)\in A} = \mathop{\sum\sum}_{g(i, j) \in A} p_{X,Y}(i,j)$
\end{hint}
\begin{solution}
\begin{align}
    \label{eq:1}
\P{L=i\given M > L} 
=\frac{\P{L=i, M > L}}{\P{M>L}}.
\end{align}

\begin{align}
\P{L=i,  M > L} 
&= 2\P{X=i,Y > X} \\
&= 2\P{X=i}\P{Y > i} \\
&= 2pq^i\cdot q^{i+1}\\
&= 2p q^{2i+1}.
\end{align}

I use three (related) ways to compute $\P{M>L}$. In the first I convert the event $\{M>L\}$ directly into a statement about $X$ and $Y$.
\begin{align}
  \label{eq:3}
\P{M>L} 
&= \P{X>Y} + \P{Y > X} \\
&= 2\P{X>Y} \\
&= 2\sum_{j} \P{X>j,Y=j} \\
&= 2\sum_{j}  \P{X>j}\P{Y=j} \\
&= 2p\sum_{j} q^{j+1} q^j\\
&= 2p\sum_{j} q^{2j+1}\\
&= 2pq\sum_{j} q^{2j}\\
&= 2pq/(1-q^2).
\end{align}
In the second,  I rewrite the event $\{M>L\}$ as a function of $X$ and $Y$, and then use  the formula of the hint:
\begin{align}
  \label{eq:4}
\P{M>L}  &= \P{\max{X, Y} > \min{X,Y}} \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} p_{XY}(i,j) \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} p_{X}(i)p_{Y}(j) \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} q^i q^j \\
&=2p^2\sum_{i}\sum_{j>i} q^i q^j \\
&=2p^2\sum_{i}q^i \sum_{j>i} q^j \\
&=2p^2\sum_{i}q^i \sum_{j=0}^\infty q^{i+1+j} \\
&=2p^2\sum_{i}q^{2i+1} \sum_{j=0}^\infty q^{j} \\
&=2p\sum_{i}q^{2i+1} \\
&=2pq\sum_{i}q^{2i} \\
&=2pq/(1-q^2).
\end{align}
In the third, I use the probability $\P{L=i, M>L}$ and take the sum over $i$:
\begin{align}
  \label{eq:48}
\P{M>L} = \sum_{i} \P{L=i, M>L}  = \sum_{i} 2pq^{2i+1} = 2pq\sum_{i} q^{2i} = \frac{2pq}{1-q^{2}}.
\end{align}

Hence, 
\begin{align}
\P{L=i\given M > L} 
&=\frac{\P{L=i, M > L}}{\P{M>L}} \\
&=\frac{2p q^{2i+1}}{2pq/(1-q^2)} \\
&=q^{2i}(1-q^2) \\
&= \P{L=i},
\end{align}
where the last equation follows since $L\sim \Geo{1-q^2}$.

\end{solution}
\end{exercise}


What have we achieved up to now? With respect to our problem, we are sure that~\eqref{eq:6} is correct when $X, Y \sim \Geo{p}$ and~\cref{eq:7} is not.
From the technical point of view, we practiced with joint, marginal, and conditional PMFs, and we have seen how to use indicator functions and the fundamental bridge.

So, this made me wonder why is~\eqref{eq:7} incorrect when $X, Y\sim\Geo{p}$, but it is correct when $X,Y\sim\Exp{\lambda}$.
To understand this discrepancy better, I decided to analyze the latter case in as much detail as I could think of, hoping that this would  provide me with a lead.
The work will be pretty technical at times, but there is no way around when dealing with joint densities, etc.



\subsection{Continuous memoryless r.v.s}
\label{sec:exerc-expon-distr}

In this section we analyze the correctness of~\cref{eq:6} and \cref{eq:7} for continuous memoryless r.v.s, i.e., exponentially distributed r.v.s.
This will offer a good opportunity to practice with several concepts: joint, marginal and conditional distributions of continuous r.v.s,  limits, and moment-generating functions.
We don't only solve the problem of~\cref{sec:confusing-problem}, in passing we will obtain a lot of necessary and useful practice.

First we need to recall some basic facts about the exponential distribution.

\begin{exercise}
Show that $X$, and hence $Y$, is memoryless.
\begin{solution}
  See BH.
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:4}
Show that $\E X = 1/\lambda$.
\begin{hint}
  Use partial integration.
\end{hint}
\begin{solution}
It is essential that  you know all elements of this solution  by heart.
  \begin{align}
\E X 
&= \int_{0}^{\infty}  x f_{X}(x) \d x  \\
&= \int_{0}^{\infty}  x \lambda e^{-\lambda x} \d x  \\
&= - xe^{-\lambda x} \biggr|_{0}^{\infty} + \int_{0}^{\infty}  e^{-\lambda x} \d x  \\
&=  - \frac 1 \lambda e^{-\lambda x} \biggr|_{0}^{\infty} = \frac 1 \lambda.
  \end{align}
Substitution is also a very important technique to solve such integrals. Here we go again:
  \begin{align}
\E X 
&= \int_{0}^{\infty}  x f_{X}(x) \d x  \\
&= \int_{0}^{\infty}  x \lambda e^{-\lambda x} \d x  \\
&= \frac 1 \lambda \int_{0}^{\infty}  u  e^{- u} \d u, 
  \end{align}
  by the substitution $u=\lambda x \implies \d u = \d (\lambda x) \implies \d u = \lambda \d x \implies \d x = \d u / \lambda$.
  With partial integration (do it!), the integral evaluates to $1$.
\end{solution}
\end{exercise}


Now we can shift our attention to the r.v.s $L$ and $M$.

\begin{exercise}
  Show that the density $f_{L}(x)$ of $L$ is equal to $2\lambda e^{-\lambda x}$.
  Use this to see that $L\sim \Exp{2\lambda}$ and that $\E L = 1/(2\lambda)$.
  Realize that with this we have an independent check of~\cref{eq:36}.
\begin{hint}
  First compute the distribution $F_L(x)$ of $L$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:22}
\P{L>x}  = \P{X>x, Y> x} = (\P{X>x})^2 = (1-\P{X\leq x})^2= (e^{-\lambda x})^2 = e^{-2\lambda x}.
  \end{align}
Hence, $F_{L}(x) = 1-e^{-2 \lambda x}$, hence $L\sim \Exp{2\lambda}$. But then, $f_L(x) = F_L'(x) = 2 \lambda e^{-2\lambda x}$. 

With infinitesimals we can get the same result, but mind, this technique requires some serious thought: 
\begin{align}
  \label{eq:24}
f_L(x) \d x 
&= \P{L \in [x, x+\d x]} \\
&= \P{X \in [x, x+\d x], Y > x}  + \P{Y \in [x, x+\d x], X > x} \\
&= f_X(x)(1-F_Y(x)) + f_Y(x) (1-F_X(x) )\d x \\
&= 2f_X(x)(1-F_Y(x)) \d x \\
&= 2 \lambda e^{-\lambda x} e^{-\lambda x} \d x  = 2 \lambda e^{-2 \lambda}\d x.
\end{align}


Thus, 
  \begin{align}
\E L
&= \int_{0}^{\infty}  x f_{L}(x) \d x  \\
&= \int_{0}^{\infty}  x 2 \lambda e^{-2\lambda x} \d x.
  \end{align}
Substitution of $u=2\lambda x$ and using the solution of~\cref{ex:4} gives the answer.

\end{solution}
\end{exercise}


\begin{exercise}\label{ex:6}
Compute $F_M(x) = \P{M\leq x}$. With this, show that  $f_{M}(x)=2(1-e^{-\lambda x}) \lambda e^{-\lambda x}$,  and then use this to compute $\E M$ to show that~\cref{eq:35} holds.
\begin{solution}
Here are two related ways. This is one:
\begin{align}
\P{M\leq x}   &= \P{X \leq x, Y \leq x} = (F_{X}(x))^{2},
\end{align}
since $X, Y$ iid.  

The second is based on~\cref{eq:55}:
\begin{align}
\P{M\leq u}   
&= \E{\1{M\leq u}} \\ 
&=\int_0^{\infty} \int_0^{\infty} \1{x\leq u, y\leq u} f_{XY}(x,y) \d x \d y \\
&= \int_0^{\infty} \int_0^{\infty} \1{x\leq u, y\leq u} f_{X}(x) f_{Y}(y) \d x \d y \\
&= \int_0^{u} f_{X}(x) \d x \int_0^{u}  f_{Y}(y)  \d y \\
&= F_{X}(u) F_Y(u) = (F_X(u))^2.
\end{align}
We get the same result.



Now  the density:
\begin{align}
  \label{eq:26}
  f_M(u) = F_M'(u) = 2 F_X(u) f_X(u) = 2(1-e^{-\lambda u}) \lambda e^{- \lambda u}.
\end{align}
Here is again my favorite method to get the density
\begin{align}
  \label{eq:28}
f_M(u) \d u = \P{M\in [u, u+\d u]}  = 2\P{X\in [u, u+\d u], Y\leq u} = 2 f_X(u) F_X(u)\d u.
\end{align}

 We use the density to  compute the expectation of $M$:
\begin{align}
  \label{eq:27}
\E M 
&= \int_{0}^{\infty} x f_M(x) \d x = \\
&= 2 \lambda \int_{0}^{\infty} x (1-e^{-\lambda x}) e^{-\lambda x} \d x = \\
&= 2 \lambda \int_{0}^{\infty} x e^{-\lambda x} \d x -  2 \lambda \int_{0}^{\infty} x e^{-2 \lambda x} \d x \\
&= 2 \E X - \E L, 
\end{align}
where the last equality follows from the previous exercises. 

\end{solution}
\end{exercise}

\begin{exercise}
We can also compute $f_{M}(y)$ (and $f_{L}(x)$) from the joint density $f_{L,M}(x,y)$ by marginalization. For this, use~\cref{eq:55} to rewrite 
\begin{equation*}
  F_{L,M}(x,y) = \P{L\leq x, M\leq y}
\end{equation*}
in terms of $F_X$ and $F_Y$.
Once you checked your answer, you might see that you forgot an important condition in the earlier exercises..
\begin{solution}
First the joint distribution.
  \begin{align}
    \label{eq:51}
F_{L,M}(x,y) &= \P{L\leq x, M \leq y} \\
&= 2\P{X \leq x, x < Y \leq y} \\
&= 2\P{X \leq x} \P{x < Y \leq y} \\
&= 2 F_{X}(x) (F_Y(y)-F_Y(x)) \1{y>x}.
  \end{align}
Note the indicator! 
Taking partial derivatives,
\begin{align}
  \label{eq:52}
f_{L,M}(x,y) 
&=\partial_x\partial_{y}F_{L,M}(x,y) \\  
&=\partial_x\partial_{y}2 F_{X}(x) (F_Y(y)-F_Y(x)) \1{y>x}\\
&=2 \partial_x \left( F_{X}(x) \partial_{y} (F_Y(y)-F_Y(x))  \1 {y>x} \right) \\
&=2 \partial_x \left( F_{X}(x) f_Y(y)  \1 {y>x} \right)\\
&= 2f_X(x) f_Y(y)\1{y>x} 
\end{align}
Note that $\partial_x$ means the same as $\frac{\partial}{\partial x}$.

Now,
\begin{align}
  \label{eq:54}
  f_M(y) &=  \int_0^{\infty} f_{L,M}(x,y) \d x \\
&= 2 \int_0^{y} f_{X}(x) f_Y(y) \d x \\
&= 2 f_Y(y) \int_0^{y} f_{X}(x) \d x  \\
&= 2f_Y(y) F_X(y), \\
f_L(x) &=  \int_0^{\infty} f_{L,M}(x,y) \d y \\
&= 2 f_{X}(x) \int_x^{\infty} f_Y(y) \d y \\
&= 2 f_{X}(x) (1-F_Y(x)).
\end{align}



\end{solution}
\end{exercise}

In Chapter 8 you will learn  how to work with transformations. Do this exercise when you read Chapter 8.
\begin{exercise}
Find a function $g$ that maps $X,Y$ to $L,M$. Then compute $f_{L,M}$ by using change of variables. 
\begin{solution}
We need $g$ first, and then we compute the Jacobian. 
\begin{align}
(u,v) = g(x, y) = (\min{x,y}, \max{x,y}).
\end{align}
Hence, $g^{-1}(u,v) = \{(u,v), (v, u)\}$; note that this occurs because $g$ is not one-to-one.
However, this is not a real problem because we can easily keep track of `each branch' of $g^{-1}$. 

Next, noting that $\partial_{x}\min{x,y} = \1{x\leq y}$, 
\begin{align}
\frac{\partial (u,v)}{\partial(x,y)} = 
  \begin{vmatrix}
    \1{x\leq y} & \1{y<x} \\
    \1{x > y} & \1{y \geq x} \\
  \end{vmatrix} = |\1{x\leq y} - \1{x>y}| = 1.
\end{align}

Now with the change of variable formula (in which I drop the Jacobian right away as it is $1$), 
\begin{align}
  \label{eq:30}
f_{L,M}(u,v) = f_{X,Y}(u,v) + f_{X,Y}(v,u)= 2f_{X,Y}(u,v)\1{u\leq v}.
\end{align}
\end{solution}
\end{exercise}

We can conclude that~\cref{eq:35} and \cref{eq:36} are correct for $X,Y\sim\Exp{\lambda}$.
But, I have yet another way to check this, namely, from~\cref{eq:7} I see that $\E{M-L} = \E X$.
Let's try to verify that also.
For this, we can first compute the conditional density $f_{M-L| L}(y| x)$; once you did the next exercise you will directly see how to compute $\E{M-L}$.
Besides this, we also have to practice with conditional densities.\footnote{And in BH Chapter 9 of the book you will learn that $\E{M-L\given L} = \E{M-L}$, and for this we also need $f_{M-L|L}$.}

\begin{exercise}\label{ex:5}
Show that $f_{M-L|L}(y| x) = \lambda e^{-\lambda y}$, i.e., $M-L| L \sim \Exp{\lambda}$.
\begin{hint}
Use Bayes' rule, and compute $f_{L, M-L}(x,y)$  first.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:29}
f_{M-L| L}(y| x) =  \frac{f_{L, M-L}(x, y)}{f_L(x)}.
  \end{align}
Clearly, we must first find $f_{L, M-L}(x, y)$. 

First I'll use the standard method.
This is a bit tricky, because we have to take care of the limits in the integration.
In itself the idea is not difficult, i.e., compute $F_{L, M-L}(x, y)$, then take partial derivatives, but the technical details require attention.
\begin{align}  \label{eq:23}
\P{L\leq x, M-L\leq y} 
&= 2\P{X\leq x, Y-X \leq y} \\
&= 2\int_0^{\infty} \int_0^{\infty} \1{u\leq x, v-u\leq y, u \leq v} f_{X,Y}(u, v)  \d u \d v\\
&= 2\int_0^{\infty} \int_0^{\infty} \1{u\leq x, v-u\leq y, u\leq v} \lambda^2e^{-\lambda u}e^{-\lambda v}\d u \d v\\
&= 2\int_0^{x} \int_0^{\infty} \1{u\leq v\leq u+y} \lambda^2e^{-\lambda u}e^{-\lambda v}\d u \d v\\
&= 2\int_0^{x} \lambda e^{-\lambda u} \int_u   ^{u + y}  \lambda e^{-\lambda v}\d v \d u\\
&= 2\int_0^{x} \lambda e^{-\lambda u} (-e^{-\lambda v}) \biggr|_u^{u+y}\d u \\
&= 2\int_0^{x} \lambda e^{-\lambda u} (e^{-\lambda u} - e^{-\lambda (u+y)})\d u \\
&= 2\lambda\int_0^{x}  e^{-2 \lambda u}\d u  - 2\lambda \int_{0}^{x} e^{-\lambda (2u+y)})\d u \\
&= 2\lambda\int_0^{x}  e^{-2 \lambda u}\d u  - 2\lambda e^{-\lambda y }\int_{0}^{x} e^{-2 \lambda u}\d u \\
&= (1-e^{-\lambda y}) 2 \lambda \int_0^{x} e^{-2 \lambda u} \d u\\ 
&= (1-e^{-\lambda y}) ( -e^{-2\lambda u}) \biggr|_0^{x} \\ 
&= (1-e^{-\lambda y})(1- e^{-2\lambda x}).
\end{align}
For the density, we need to take the partial derivatives with respect to $x$ and $y$: 
\begin{align}
  \label{eq:25}
f_{L, M-L}(x,y ) 
&= \partial_x\partial_{y} F_{L, M-L}(x,y)   \\
&= \partial_x\partial_{y} (1-e^{-\lambda y})(1- e^{-2\lambda x}) \\
&= \lambda e^{-\lambda y} 2\lambda  e^{-2\lambda x} \\
&= f_Y(x) f_L(y).
\end{align}

If we plug this into~\cref{eq:29} we see that
  \begin{align}
f_{M-L| L}(y| x) =  f_Y(y) = \lambda e^{-\lambda y}
  \end{align}
We arrive at  the same result as in~\cref{eq:30}. 

\end{solution}
\end{exercise}

\begin{exercise}
Use the solution of~\cref{ex:5} to show that $\E{M-L} = \E X$.
\begin{hint}
Use the expression of $f_{L, M-L}(x,y)$  to find the density of $f_{M-L}(y)$.
\end{hint}
\begin{solution}
  \begin{align}
\E{M-L} 
&= \int_{0}^{\infty} x f_{M-L}(x) \d x.
  \end{align}
Next, 
\begin{align}
  \label{eq:32}
f_{M-L}(y) 
&= \int_0^\infty f_{L, M-L}(x, y) \d x  = \int_0^\infty f_{L}(x) f_Y(y) \d x = f_Y(y) \int_0^{\infty }f_L(x) \d x = f_Y(y).
\end{align}
Hence, 
\begin{align}
  \label{eq:33}
  \E{M-L} = \int_0^{\infty} y f_Y(y) \d y = \E Y = \E X. \quad X, Y \iid
\end{align}

\end{solution}
\end{exercise}

\begin{exercise}
Here is another opportunity to check our work, but now with changing variables, cf. Chapter 8.
Find $f_{L,M-L}(x,y)$ by means of changing of variables from $L,M$ to $L,M-L$.
\begin{solution}
  The function $h(x,y)= (x, y-x)\1{x\leq y}$ maps $(L,M)$ to $(L, M-L)$. Then, $h^{-1}(u,v) = (u, u+v)$. For the Jacobian
  \begin{align}
    \label{eq:57}
\frac{\partial (u,v)}{\partial(x,y)} = 
    \begin{vmatrix}
      \1{x\leq y} & 0 \\
-\1{x\leq y} & \1{x\leq y}
    \end{vmatrix} = \1{x\leq y}.
  \end{align}
And with this,
\begin{align}
  \label{eq:19}
f_{L, M-L}(u,v) 
&= f_{L, M}(u, u+v) \1{u\leq u+v}  \\
&= f_{L,M}(u, u+v) \\
&= 2 f_{X}(u) f_Y(u+v) \\
&= 2 \lambda^{2}e^{-2u} e^{-\lambda v} \\
&= f_{L}(u) f_Y(v).
\end{align}

\end{solution}
\end{exercise}


We did a number of checks for the case $X, Y, \iid, \sim\Exp{\lambda}$.
But I have yet another idea to see whether the results we have obtained so far are consistent.
For this I use BH Ex 5.43 in which it is shown that the geometric distribution is the discrete analog of the exponential distribution. Now I want to see how, by taking proper limits, the results of this section can be obtained as limiting cases of those of~\cref{sec:set-1}. 

More specically, I want to see intuitively\footnote{A formal proof is too hard for this course.} how $X\sim \Geo{\lambda/n}$ approaches  $Y\sim\Exp{\lambda}$ as $n\to\infty$.

\begin{exercise}\label{ex:7} 
Divide the interval $[0, \infty)$ into many small intervals of length $1/n$.
Let  $X\sim \Geo{\lambda/n}$ for some $\lambda>0$ and $n\gg 0$, take  some $x\geq 0$,  let $i$ be such that $x\in[i/n, (i+1)/n)$.
Then show that 
\begin{align}\label{eq:53}
\P{X/n \approx x} \approx \frac{\lambda} n \left(1-\frac \lambda n\right)^{xn}.
\end{align}
\begin{solution}
First, 
\begin{align}
\P{X/n \approx x} &= \P{X/n \in [i/n, (i+1)/n]} = \P{X \in [i, i+1]} = p q^{i} \approx p q^{n x}.
\end{align}
Now take $p=\lambda/n$ and $q=1-p=1-\lambda/n$, then,
\begin{equation}
\P{X/n \approx x} \approx \frac{\lambda} n \left(1-\frac \lambda n\right)^{xn}.
\end{equation}
\end{solution}
\end{exercise}
From BH.A.2.5, $\lim_{n\to\infty}(1-\lambda/n)^{n} = e^{-\lambda}$, which is a standard limit but, interestingly, not entirely trivial to prove.\footnote{If you like mathematics, check the neat proof in Rudin's Principles of mathematical analysis.}
Next, by introducing the highly intuitive definition $\d x = \lim_{n\to \infty} 1/n$, it follows from~\cref{eq:53} that 
\begin{align}
\P{X/n \approx x} \to \lambda e^{-\lambda x} \d x,\quad\text{ as } n \to \infty.
\end{align}
In fact, working with infinitesimals such as $\d x$ is one of my favorite methods, for instance to derive densities.
The intuition is that $\d x$ is some really small number but not zero.
In a tiny bit more formal words, $0<\d x < 1/n$ for any $n$, but $\d x \neq 0$.
For details, check out the web; search on `nonstandard calculus'.
However, while the method is very intuitive, it's easy to mess up. (Power tools are nice, but often only when you know how to handle them and practice a lot.) 
If you don't want to learn this, no problem, it will not be part of the course proper.

In fact, if you don't like infinitesimals, it is also possible obtain this limit with moment-generating functions (and we can practice with this function again).

\begin{exercise}
Derive the moment-generating function $M_{X/n}(s)$ of $X/n$ when $X\sim \Geo{p}$.
Then, let $p = \lambda/n$, and show that $\lim_{n\to\infty}M_{X/n}(s) = M_{Y}(s)$, where $Y \sim \Exp{\lambda}$.
\begin{hint}
  Use that, as in the Poisson distribution, $e^{-\lambda} = \sum_{i=0}^{\infty}\lambda^{i}/i!$.
  In fact, this precisely Taylor's expansion of $e^{s/n}$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:20}
M_{X/n}(s) 
&= \E{e^{s X/n}} = \sum_{i} e^{s i/n} p q^{i} = p\sum_{i} (qe^{s/n})^{i} = \frac{p}{1-qe^{s/n}}.
  \end{align}
With $p=\lambda/n$ this becomes
  \begin{align}
M_{X/n}(s) 
&= \frac{\lambda/n}{1-(1-\lambda/n) (1+s/n + 1/n^{2}\times(\cdots))} \\
&= \frac{\lambda/n}{\lambda/n - s/n + 1/n^{2}\times (\cdots)} \\
&= \frac{\lambda}{\lambda - s + 1/n\times(\cdots)} \\
&\to \frac{\lambda}{\lambda - s}, \quad\text{as }  n\to \infty,
  \end{align}
  where we write $1/n^{2}\times(\cdots)$ for all terms that will disappear when we take the limit $n\to \infty$.
  This is just handy notation to hide details in which we are not interested.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:11}
Check that $\lim_{n\to\infty}\E{L/n} = 1/(2\lambda)$ when $X\sim \Geo{\lambda/n}$. Conclude that  we retrieve~\cref{eq:36}.
\begin{hint}
  Use~\cref{ex:3}.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:21}
\E{L/n}  &= \frac 1 n \E L = \frac 1 n \frac{q^2}{1-q^{2}} \\
 &= \frac 1 n \frac{(1-\lambda/n)^2}{1-(1-\lambda/n)^{2}} \\
 &= \frac 1 n \frac{1-2 \lambda/n + (\lambda/n)^2}{2 \lambda /n + (\lambda/n)^{2}} \\
 &= \frac{1-2 \lambda/n + (\lambda/n)^2}{2 \lambda + \lambda^{2}/n} \\
&\to \frac 1{2\lambda}.
  \end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Here is yet another check on the correctness of $f_M(x)$. 
Show that  the PMF $\P{M=k}$ of~\cref{eq:34} converges to $f_M(x)$ of~\cref{ex:6} when $n\to \infty$. Take $k$ suitable.
\begin{solution} Take $p=\lambda/n$,  $q=1-\lambda/n$, and $k \approx x n$, hence $k/n \approx x$. Then,
  \begin{align}
    \label{eq:31}
\P{M/n=k/n}  
&= 2 p q^{k/n}(1-q^{k/n}) + p^2q^{2k/n} \\
&= 2 \frac \lambda n \left(1-\frac \lambda n\right)^{k/n}\left(1-\left(1-\frac \lambda n\right)^{k/n}\right) + \frac{\lambda^2}{n^2}\left(1-\frac \lambda n \right)^{2k/n} \\
&\to 2 \lambda \d x e^{-\lambda x} \left(1 - e^{-\lambda x}\right) + \lambda^{2} \d x^{2}e^{-2\lambda}.\label{eq:37}
  \end{align}
Now observe that the second term, proportional to $\d x^{2}$ can be neglected. 
\end{solution}
\end{exercise}

All in all, we have checked and double checked all our expressions and limits for the geometric and exponential distribution.
We had success too: the solution of the last exercise provides the key to understand why~\eqref{eq:6} and \eqref{eq:7} are true for exponentially distributed r.v.s, but not for geometric random variables.
In fact, in the solutions we can see the term corresponding to $X=Y=i$ for $X,Y\sim \Geo{p}$ becomes negligibly small when $n\to 0$.
In words, the probability that $X$ and $Y$ are the same is non-negligible when they are discrete, but it is when $X$ and $Y$ are continuous.
So, to resolve our leading problem we should exploit that idea.

\subsection{The solution}
\label{sec:solution}


Let us now try to repair~\cref{eq:7}  for the case $X,Y\sim\Geo{p}$. We should be careful about the non-negligible case that $M=L$, so we move on, carefully, step by step. The following must be true:
\begin{align}
  \label{eq:40}
\E M &= \E L + \E{(M-L) \1{M>L}}  = \E L + 2\E{(Y-X) \1{Y>X}},
\end{align}
because either $M=L$ or $M>L$.
Recall from earlier work that the factor 2 in the second equality follows from the fact that $X,Y$ iid.

\begin{exercise}\label{ex:9}
Show that 
\begin{equation}
  \label{eq:41}
2\E{(Y-X) \1{Y>X}} = \frac{2q} {1-q^{2}}.
\end{equation}
Combine this with the expression for $\E L$ of~\cref{ex:3} to obtain \cref{eq:10} for $\E M = 2\E X - \E L$, thereby verifying the correctness of~\cref{eq:40}. 
\begin{solution}
  \begin{align}
    \label{eq:42}
\E{(Y-X) \1{Y>X}} 
&= p^2\sum_{ij} (j-i)\1{j>i} q^iq^j\\
&= p^2\sum_{i} q^i\sum_{j=i+1}^{\infty}(j-i)q^j\\
&= p^2\sum_{i} q^i q^{i}\sum_{k=1}^{\infty}kq^k, \quad k = j-i\\
&= p\sum_{i}  q^{2i}\E X \\
&= \frac{p}{1-q^{2}}\E X \\
&= \frac{p}{1-q^{2}}\frac q p \\
&= \frac{q}{1-q^{2}}.
  \end{align}
With this, 
\begin{align}
  \label{eq:43}
\E L + 2  \E{(Y-X) \1{Y>X}} = \frac{q^{2}}{1-q^2} +  \frac{2q}{1-q^{2}} = \frac q{1-q}\frac{q+2}{1+q},
\end{align}
where I use that $1-q^{2}=(1-q)(1+q)$.
\end{solution}
\end{exercise}

While~\cref{eq:40} is correct, I am still not happy with the second part of~\eqref{eq:40} (I find this hard/unintuitive to interpret).
Restarting again from scratch, here is another attempt to rewrite $\E M$: 
\begin{align}
  \label{eq:44}
\E M = \E L + \E{ Z \1{M>L}},
\end{align}
where I take $Z\sim \FS{p}$, i.e., $Z$ has the first success distribution with parameter $p$, in other words, $Z \sim X+1$ with $X\sim\Geo{p}$.
To see why this might be true, I reason like this.
After `seeing' $L$, we want to restart.
Let $Z$ be the time from the restart to $M$.
When $Z\sim \Geo{p}$, it might happen that $Z=0$ (with positive probability $p$).
But if $Z=0$, then $M=L$, and it that case, we should not restart.
Hence, if $Z\sim \Geo{p}$ we are `double counting' when $Z=0$.
By including the condition $M>L$ and by taking $Z\sim\FS{p}$ (so that $Z>0$) I can prevent this.

\begin{exercise}
Show that 
\begin{equation}
  \label{eq:45}
\E{Z\1{M>L}} = \frac{2q} {1-q^{2}},
\end{equation}
i.e., the same as~\cref{eq:41}, hence~\cref{eq:44} is correct.
\begin{hint}
Observe that $Z$   is independent from $X$ and $Y$, hence from $M$ and $L$
\end{hint}
\begin{solution} With the hint:
  \begin{align}
    \label{eq:46}
\E{Z\1{M>L}}  = \E Z \E{\1{M>L}} = \E Z \P{M>L} = \frac 1 p \frac{2pq}{1-q^{2}} = \frac{2q}{1-q^{2}},
  \end{align}
where, from the book, $\E Z = 1/p$, and $\P{M>L}$ follows from~\cref{ex:10}. 
\end{solution}
\end{exercise}


I am nearly happy, but I want to see that~\eqref{eq:44}, which is correct for discrete r.v.s, has also the correct limiting behavior, similar to~\cref{ex:11}. 
\begin{exercise}
Show that $\E{Z/n\1{M>L}} \to 1/\lambda$, which is indeed the expectation of an $\Exp{\lambda}$ r.v. And thus, when $X,Y\sim \Exp{\lambda}$, $\E M = \E L + \E X$.
\begin{solution}
By independence,
  \begin{align}
    \label{eq:47}
\E{Z/n\1{M>L}} = \E{Z/n} \P{M>L}. 
  \end{align}
Then,
\begin{align}
  \label{eq:49}
\P{M>L}   = \frac{2pq}{1-q^{2}} 
= \frac{2\lambda/n (1-\lambda/n)}{1-(1-\lambda/n)^{2}} = 
= \frac{2\lambda/n (1-\lambda/n)}{2\lambda/n -\lambda^{2}/n^{2}}  
= \frac{2 (1-\lambda/n)}{2 -\lambda/n}  \to 1,
\end{align}
and
\begin{align}
  \label{eq:50}
\E{Z/n} = \frac 1 n \E Z = \frac 1 n \frac 1 p = \frac 1 {n \lambda/ n} = 1/\lambda.
\end{align}
\end{solution}

\end{exercise}

It took me a long time, and a lot of work, to understand how to resolve the confusing problem.
Perhaps how to tackle this problem is clear to any other person, but I had a nice time working on it, and I learned a lot.
In particular, I find~\cref{eq:44} a nice and revealing equation.




\opt{all-solutions-at-end}{
\clearpage
\Closesolutionfile{hint}
\clearpage
\Closesolutionfile{ans}
\section{Hints}
\input{hint}
\section{Solutions}
\input{ans}
}



\end{document}
